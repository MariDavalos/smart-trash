import os
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# ============ CONFIGURACIÃ“N INICIAL ============
# ConfiguraciÃ³n de GPU (automÃ¡tica si estÃ¡ disponible)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("âœ… GPU detectada y configurada")
    except RuntimeError as e:
        print(f"âš ï¸ Error al configurar GPU: {e}")

# ============ FUNCIÃ“N PARA CARGAR EL DATASET ============
def load_dataset(data_path="../dataset", img_size=(128, 128), test_size=0.2, batch_size=32, max_samples_per_class=None):
    """
    Carga el dataset optimizado para evitar problemas de memoria.
    
    Args:
        data_path (str): Ruta al dataset.
        img_size (tuple): TamaÃ±o de redimensionamiento (alto, ancho).
        test_size (float): Porcentaje para datos de prueba.
        batch_size (int): TamaÃ±o del lote para procesamiento.
        max_samples_per_class (int): LÃ­mite de imÃ¡genes por clase (opcional).
    
    Returns:
        train_dataset (tf.data.Dataset): Datos de entrenamiento.
        test_dataset (tf.data.Dataset): Datos de prueba.
        class_names (list): Nombres de las clases.
    """
    # DefiniciÃ³n de clases
    classes = {
        'papel': 0,
        'organico': 1,
        'metal': 2,
        'plastico': 3
    }
    
    print("\n=== SMART TRASH - DATA PREPROCESSING ===")
    print(f"ğŸ” Buscando dataset en: {os.path.abspath(data_path)}")
    print(f"ğŸ–¼ï¸ TamaÃ±o de imagen: {img_size}")
    print(f"ğŸ“¦ MÃ¡ximo de imÃ¡genes por clase: {max_samples_per_class or 'Sin lÃ­mite'}\n")

    # VerificaciÃ³n de estructura del dataset
    missing_folders = [cls for cls in classes if not os.path.exists(os.path.join(data_path, cls))]
    if missing_folders:
        raise FileNotFoundError(f"âŒ Carpetas faltantes: {missing_folders}")

    # Paso 1: Recopilar rutas de imÃ¡genes y etiquetas
    image_paths = []
    labels = []
    
    for class_name, label in classes.items():
        class_dir = os.path.join(data_path, class_name)
        class_count = 0
        
        # Recorrer todas las subcarpetas
        for root, _, files in os.walk(class_dir):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    if max_samples_per_class and class_count >= max_samples_per_class:
                        break
                    image_paths.append(os.path.join(root, file))
                    labels.append(label)
                    class_count += 1
        
        print(f"ğŸ“‚ {class_name}: {class_count} imÃ¡genes")
        if class_count == 0:
            print(f"   âš ï¸ Â¿Las imÃ¡genes estÃ¡n en subcarpetas? Ej: '{class_name}/subcarpeta/imagen.png'")

    if not image_paths:
        raise ValueError("âŒ No se encontraron imÃ¡genes vÃ¡lidas (.png/.jpg/.jpeg)")

    # Paso 2: Dividir en train/test (a nivel de rutas, no datos)
    train_paths, test_paths, train_labels, test_labels = train_test_split(
        image_paths, labels, test_size=test_size, random_state=42, stratify=labels
    )

    # Paso 3: Crear un generador de datos eficiente (tf.data.Dataset)
    def preprocess_image(path, label):
        img = tf.io.read_file(path)
        img = tf.image.decode_image(img, channels=3, expand_animations=False)
        img = tf.image.resize(img, img_size)
        img = img / 255.0  # NormalizaciÃ³n [0, 1]
        return img, label

    # Convertir etiquetas a one-hot encoding
    train_labels = to_categorical(train_labels, num_classes=len(classes))
    test_labels = to_categorical(test_labels, num_classes=len(classes))

    # Crear datasets de TensorFlow (carga perezosa)
    train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))
    train_dataset = train_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))
    test_dataset = test_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    print("\nâœ… Â¡Dataset cargado exitosamente!")
    print(f"ğŸ“Š EstadÃ­sticas:")
    print(f" - Total imÃ¡genes: {len(image_paths)}")
    print(f" - Entrenamiento: {len(train_paths)}")
    print(f" - Prueba: {len(test_paths)}")
    print(f" - TamaÃ±o de lote: {batch_size}")

    return train_dataset, test_dataset, list(classes.keys())

# ============ EJECUCIÃ“N PRINCIPAL ============
if __name__ == "__main__":
    try:
        # ConfiguraciÃ³n recomendada para evitar problemas de memoria
        train_data, test_data, class_names = load_dataset(
            data_path="../dataset",
            img_size=(64, 64),  # Reducir si hay problemas de RAM
            batch_size=16,
            max_samples_per_class=2000  # Opcional: limita imÃ¡genes por clase
        )

        # Ejemplo de modelo simple (para probar el dataset)
        model = tf.keras.Sequential([
            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
            tf.keras.layers.MaxPooling2D(),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(4, activation='softmax')
        ])

        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        model.fit(train_data, validation_data=test_data, epochs=2)  # Prueba rÃ¡pida

    except Exception as e:
        print(f"\nâŒ Error durante la ejecuciÃ³n: {str(e)}")
        print("\nğŸ”§ Posibles soluciones:")
        print("1. Verifica que todas las carpetas (papel, organico, metal, plastico) existen")
        print("2. AsegÃºrate de que las imÃ¡genes estÃ¡n en subcarpetas (ej: plastico/botellas/img1.png)")
        print("3. Reduce 'img_size' o 'max_samples_per_class' si hay problemas de RAM")
        print("4. Ejecuta desde la carpeta 'training/'")